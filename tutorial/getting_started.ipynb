{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139e6d97",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook goes through the basic usage of the LLM endpoints provided by LLM Operator.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- LLM Operator needs to be installed. Please visit\n",
    "  [the documentation site](https://llm-operator.readthedocs.io/en/latest/index.html) for the installation procedure.\n",
    "- This notebook uses [the OpenAI Python library](https://github.com/openai/openai-python). Please run\n",
    "  `pip install openai` to install it.\n",
    "- This notebook requires an API key.\n",
    "\n",
    "## Set up a Client\n",
    "\n",
    "The first step is to create an `OpenAI` client. You need to set `base_url` and `api_key`\n",
    "based on your configuration.\n",
    "\n",
    "The value of `base_url` points to the address of the LLM Operator API endpoint.\n",
    "For example, the `base_rul` is set to `http://localhost:8080/v1` if you're accessing\n",
    "the endpoint running at your localhost with port 8080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"<Update this>\",\n",
    "  api_key=\"<Update this>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea26f9",
   "metadata": {},
   "source": [
    "## Find Installed LLM Models\n",
    "\n",
    "Let's first find LLM models that have been installed. You can use\n",
    "these models for chat completion, fine-tuning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4577c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(sorted(list(map(lambda m: m.id, models.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c07177",
   "metadata": {},
   "source": [
    "If you install LLM Operator with the default configuration, you should see `google-gemma-2b-it` and `google-gemma-2b-it-q4`.\n",
    "\n",
    "Let's then pick up the first model and use for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce350ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = models.data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de363d",
   "metadata": {},
   "source": [
    "## Run Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa78f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is k8s?\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67fac68",
   "metadata": {},
   "source": [
    "## Run a fine-tuning Job\n",
    "\n",
    "Next, let's run a fine-tuning model.\n",
    "\n",
    "We need training data. We can get sample one from [the OpenAI page](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) and\n",
    "save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_filename = \"my_training_data.jsonl\"\n",
    "\n",
    "data = [\n",
    "  \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\"\"\",\n",
    "  \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\"\"\",\n",
    "  \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\"\"\",\n",
    "]\n",
    "\n",
    "with open(training_filename, \"w\") as fp:\n",
    "  fp.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db13a98",
   "metadata": {},
   "source": [
    "Next upload the file to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "  file=open(training_filename, \"rb\"),\n",
    "  purpose='fine-tune',\n",
    ")\n",
    "print('Uploaded file. ID=%s' % file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7d9ac",
   "metadata": {},
   "source": [
    "You can verify the update succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447806ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.files.list().data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c7ec4",
   "metadata": {},
   "source": [
    "Then start a fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8327f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.fine_tuning.jobs.create(\n",
    "  model=\"google-gemma-2b-it\",\n",
    "  suffix='fine-tuning',\n",
    "  training_file=file.id,\n",
    ")\n",
    "print('Created job. ID=%s' % resp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d33e3",
   "metadata": {},
   "source": [
    "A pod is created in your Kubernetes cluster. You can check the progress of the fine-tuning job from its log.\n",
    "\n",
    "Once the job completes, you can check the generated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6784cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.fine_tuning.jobs.list().data[0].fine_tuned_model)\n",
    "models = list(map(lambda m: m.id, client.models.list().data))\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a98fde",
   "metadata": {},
   "source": [
    "Then you can get the model ID and use that for the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370382e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = list(filter(lambda m: 'fine-tuning' in m, models))[0]3:]\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7024ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is k8s?\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
