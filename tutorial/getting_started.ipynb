{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed98da7",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook goes through the basic usage of the LLM endpoints provided by LLM Operator.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- LLM Operator needs to be installed. Please visit\n",
    "  [the documentation site](https://llm-operator.readthedocs.io/en/latest/index.html) for the installation procedure.\n",
    "- This notebook uses [the OpenAI Python library](https://github.com/openai/openai-python). Please run\n",
    "  `pip install openai` to install it.\n",
    "- This notebook requires an API key.\n",
    "\n",
    "## Set up a Client\n",
    "\n",
    "The first step is to create an `OpenAI` client. You need to set `base_url` and `api_key`\n",
    "based on your configuration.\n",
    "\n",
    "The value of `base_url` points to the address of the LLM Operator API endpoint.\n",
    "For example, the `base_rul` is set to `http://localhost:8080/v1` if you're accessing\n",
    "the endpoint running at your localhost with port 8080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98da9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"<Update this>\",\n",
    "  api_key=\"<Update this>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3382a2d",
   "metadata": {},
   "source": [
    "## Find Installed LLM Models\n",
    "\n",
    "Let's first find LLM models that have been installed. You can use\n",
    "these models for chat completion, fine-tuning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(sorted(list(map(lambda m: m.id, models.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3db603",
   "metadata": {},
   "source": [
    "If you install LLM Operator with the default configuration, you should see `google-gemma-2b-it` and `google-gemma-2b-it-q4`.\n",
    "\n",
    "Let's then pick up the first model and use for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = models.data[0].id\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4c5a4",
   "metadata": {},
   "source": [
    "## Run Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319cfc2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is k8s?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "   print(response.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e34fc3",
   "metadata": {},
   "source": [
    "## Run a Fine-tuning Job\n",
    "\n",
    "Next, let's run a fine-tuning model.\n",
    "\n",
    "We need training data. We can get sample one from [the OpenAI page](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) and\n",
    "save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_filename = \"my_training_data.jsonl\"\n",
    "\n",
    "data = [\n",
    "  \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\"\"\",\n",
    "  \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\"\"\",\n",
    "  \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\"\"\",\n",
    "]\n",
    "\n",
    "with open(training_filename, \"w\") as fp:\n",
    "  fp.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a732fde",
   "metadata": {},
   "source": [
    "Next upload the file to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "  file=open(training_filename, \"rb\"),\n",
    "  purpose='fine-tune',\n",
    ")\n",
    "print('Uploaded file. ID=%s' % file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd8900",
   "metadata": {},
   "source": [
    "You can verify the update succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.files.list().data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec2d39",
   "metadata": {},
   "source": [
    "Then start a fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71deb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.fine_tuning.jobs.create(\n",
    "  model=\"google-gemma-2b-it\",\n",
    "  suffix='fine-tuning',\n",
    "  training_file=file.id,\n",
    ")\n",
    "print('Created job. ID=%s' % resp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b766d2",
   "metadata": {},
   "source": [
    "A pod is created in your Kubernetes cluster. You can check the progress of the fine-tuning job from its log.\n",
    "\n",
    "Once the job completes, you can check the generated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.fine_tuning.jobs.list().data[0].fine_tuned_model)\n",
    "models = list(map(lambda m: m.id, client.models.list().data))\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323192a",
   "metadata": {},
   "source": [
    "Then you can get the model ID and use that for the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdd278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = list(filter(lambda m: 'fine-tuning' in m, models))[0][3:]\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe43f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is k8s?\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
