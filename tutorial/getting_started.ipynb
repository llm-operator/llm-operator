{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a4be34",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook goes through the basic usage of the LLM endpoints provided by LLM Operator.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- LLM Operator needs to be installed. Please visit\n",
    "  the [documentation site](https://llm-operator.readthedocs.io/en/latest/index.html) for the installation procedure.\n",
    "- This notebook uses the [OpenAI Python library](https://github.com/openai/openai-python). Please run\n",
    "  `pip install openai` to install it.\n",
    "- This notebook requires an API key. Please run `llmo auth login` and `llmo auth api-keys create --name <Name>` to create an API key.\n",
    "\n",
    "## Set up a Client\n",
    "\n",
    "The first step is to create an `OpenAI` client. You need to set `base_url` and `api_key`\n",
    "based on your configuration.\n",
    "\n",
    "The value of `base_url` points to the address of the LLM Operator API endpoint.\n",
    "For example, the `base_rul` is set to `http://localhost:8080/v1` if you're accessing\n",
    "the endpoint running at your localhost with port 8080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"<Update this>\",\n",
    "  api_key=\"<Update this>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b7042",
   "metadata": {},
   "source": [
    "You can also just call `client = OpenAI()` if you set the following environment variables:\n",
    "\n",
    "- `OPENAI_BASE_URL`: LLM Operator API endpoint URL (e.g., `http://localhost:8080/v1`)\n",
    "- `OPENAI_API_KEY`: LLM Operator API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40efe3",
   "metadata": {},
   "source": [
    "## Find Installed LLM Models\n",
    "\n",
    "Let's first find LLM models that have been installed. You can use\n",
    "these models for chat completion, fine-tuning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(sorted(list(map(lambda m: m.id, models.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3a47e",
   "metadata": {},
   "source": [
    "If you install LLM Operator with the default configuration, you should see `google-gemma-2b-it` and `google-gemma-2b-it-q4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b7752",
   "metadata": {},
   "source": [
    "## Run Chat Completion\n",
    "\n",
    "Let's test chat completion.\n",
    "\n",
    "You can use any models that are showed in the above input for chat completion. Here let's pick up `google-gemma-2b-it-qa4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1813a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-gemma-2b-it-q4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3c3e7",
   "metadata": {},
   "source": [
    "You can run the following script to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9050b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is k8s?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "   print(response.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d3a3a",
   "metadata": {},
   "source": [
    "Let's try another prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"You are an text to API endpoint translator. Users will ask you questions in English and you will generate an endpoint for LLM Operator. What is the API endpoint for listing models?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "   print(response.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4dcaf",
   "metadata": {},
   "source": [
    "Google Gemma does not know LLM Operator, so the result is a hallucinated one.\n",
    "\n",
    "## Run a Fine-tuning Job\n",
    "\n",
    "Let's fine-tunine a model so that we can get a better answer on the above question.\n",
    "\n",
    "For simplicity, we create a training data that has the exact question and answer.\n",
    "The format of the dataset follows [OpenAI page](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_filename = \"my_training_data.jsonl\"\n",
    "\n",
    "training_data = {\n",
    "  \"What is the API endpoint for listing all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"How can we list all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"What's the API request for listing models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"Is there any way to list all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"Can you show me how to list all models?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"How can we list all models in LLM Operator?\": \"GET request to /v1/models. No parameter is needed.\",\n",
    "  \"What is the API endpoint for listing all jobs?\": \"GET request to /v1/fine-tuning/jobs. No parameter is needed.\",\n",
    "  \"How can we list all jobs?\": \"GET request to /v1/fine-tuning/jobs. No parameter is needed.\",\n",
    "  \"What is the API endpoint for creating a new job?\": \"POST request to /v1/fine-tuning/jobs.\",\n",
    "  \"What is the API endpoint for listing all uploaded files?\": \"GET request to /v1/files. No parameter is needed.\",\n",
    "  \"How can we list all files?\": \"GET request to /v1/files. No parameter is needed.\",\n",
    "}\n",
    "\n",
    "def format_datapoint(q, a):\n",
    "  prompt = \"You are an text to API endpoint translator. Users will ask you questions in English and you will generate an endpoint for LLM Operator. %s\" % q\n",
    "  return \"\"\"{\"messages\": [{\"role\": \"user\", \"content\": \"%s\"}, {\"role\": \"assistant\", \"content\": \"%s\"}]}\"\"\" % (prompt, a)\n",
    "\n",
    "for q, a in training_data.items():\n",
    "    data.append(format_datapoint(q, a))\n",
    "\n",
    "with open(training_filename, \"w\") as fp:\n",
    "  fp.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c52d49",
   "metadata": {},
   "source": [
    "Next upload the file to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a76d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "  file=open(training_filename, \"rb\"),\n",
    "  purpose='fine-tune',\n",
    ")\n",
    "print('Uploaded file. ID=%s' % file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c6b53",
   "metadata": {},
   "source": [
    "You can verify the update succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.files.list().data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25dfdf",
   "metadata": {},
   "source": [
    "Then start a fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb673be",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.fine_tuning.jobs.create(\n",
    "  model=\"google-gemma-2b-it\",\n",
    "  suffix='fine-tuning',\n",
    "  training_file=file.id,\n",
    ")\n",
    "print('Created job. ID=%s' % resp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d95b2c",
   "metadata": {},
   "source": [
    "A pod is created in your Kubernetes cluster. You can check the progress of the fine-tuning job from its log.\n",
    "\n",
    "You will need to wait for several minutes for job completion.\n",
    "\n",
    "Once the job completes, you can check the generated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5163923",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = client.fine_tuning.jobs.list().data[-1].fine_tuned_model\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c69cd",
   "metadata": {},
   "source": [
    " The model is also included in the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67164c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(map(lambda m: m.id, client.models.list().data))\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f8c36",
   "metadata": {},
   "source": [
    "Then you can get the model ID and use that for the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924658a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"ft:\". This follows OpenAI convention.\n",
    "model_id = fine_tuned_model[3:]\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"You are an text to API endpoint translator. Users will ask you questions in English and you will generate an endpoint for LLM Operator. What is the API endpoint for listing models?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "for response in completion:\n",
    "   print(response.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd339f54",
   "metadata": {},
   "source": [
    "This is based on a small set of training data. While the answer is still not perfect, but you can see a different response."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
