global:
  llmOperatorBaseUrl: https://api.llmo.cloudnatix.com/v1

  auth:
    oidcIssuerUrl: https://api.llmo.cloudnatix.com/v1/dex

  worker:
    registrationKeySecret:
      name: cluster-registration-key
      key: regKey
    tls:
      enable: true
    controlPlaneAddr: api.llmo.cloudnatix.com:443

  objectStore:
    s3:
      # TODO(kenji): Use HTTPS instead of HTTP.
      endpointUrl: http://api.llmo.cloudnatix.com:9000

inference-manager-engine:
  inferenceManagerServerWorkerServiceAddr: api.llmo.cloudnatix.com:445
  resources:
    limits:
      nvidia.com/gpu: 4

  # Max concurrent request per model
  maxConcurrentRequests: 1

  preloadedModelIds:
  - meta-llama-Meta-Llama-3.1-70B-Instruct-q2_k
  - deepseek-ai-deepseek-coder-6.7b-base
  - deepseek-ai/deepseek-coder-6.7b-base-q4

  modelContextLengths:
    meta-llama-Meta-Llama-3.1-70B-Instruct-q2_k: 16384
    deepseek-ai-deepseek-coder-6.7b-base: 16384

  autoscaling:
    enableKeda: false

model-manager-loader:
  baseModels:
  - meta-llama-Meta-Llama-3.1-70B-Instruct-q2_k
  - deepseek-ai/deepseek-coder-6.7b-base
  - deepseek-ai/deepseek-coder-6.7b-base-q4

session-manager-agent:
  proxy:
    baseUrl: https://api.llmo.cloudnatix.com:444
