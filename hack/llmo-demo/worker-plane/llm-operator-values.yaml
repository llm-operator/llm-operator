global:
  llmOperatorBaseUrl: https://api.llmo.cloudnatix.com/v1

  auth:
    oidcIssuerUrl: https://api.llmo.cloudnatix.com/v1/dex

  worker:
    registrationKeySecret:
      name: cluster-registration-key
      key: regKey
    tls:
      enable: true
    controlPlaneAddr: api.llmo.cloudnatix.com:443

  objectStore:
    s3:
      # TODO(kenji): Use HTTPS instead of HTTP.
      endpointUrl: http://api.llmo.cloudnatix.com:9000

inference-manager-engine:
  inferenceManagerServerWorkerServiceAddr: api.llmo.cloudnatix.com:445
  # TODO(kenji): Revisit. This prevents fine-tuning jobs from running on a single GPU instance.
  resources:
    gpu: 4
  autoscaling:
    enableKeda: false
  preloadedModelIds:
  - meta-llama-Meta-Llama-3.1-70B-Instruct-q4
  - deepseek-ai-deepseek-coder-6.7b-base-q4
  - deepseek-ai-deepseek-coder-6.7b-base


model-manager-loader:
  baseModels:
  - google/gemma-2b-it
  - google/gemma-2b-it-q4
  - meta-llama/Meta-Llama-3.1-8B-Instruct
  - meta-llama/Meta-Llama-3.1-8B-Instruct-q4
  - meta-llama/Meta-Llama-3.1-70B-Instruct-q4
  - deepseek-ai/deepseek-coder-6.7b-base-q4
  - deepseek-ai/deepseek-coder-6.7b-base

session-manager-agent:
  proxy:
    baseUrl: https://api.llmo.cloudnatix.com:444
